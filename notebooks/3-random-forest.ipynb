{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43052e37",
   "metadata": {},
   "source": [
    "# Woolsey Fire Model\n",
    "\n",
    "The model defined in this notebook seeks to model how pre-fire conditions—including vegetation, topography, and climate—contributed to burn severity during the Woolsey Fire. The configuration of the model can be modified under the **Configure model** section, and the results are saved to the **models** directory in the **woolsey_fire** folder when the run finishes. \n",
    "\n",
    "This notebook is based on an example developed by Leah Wasser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a52ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, SMOTENC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import mode\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.filters import gaussian\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    mean_squared_error\n",
    ")\n",
    "import xarray as xr\n",
    "\n",
    "from ea_drought_burn.config import DATA_DIR\n",
    "from ea_drought_burn.notebooks import run_notebook\n",
    "from ea_drought_burn.utils import (\n",
    "    aggregate,\n",
    "    agg_to_raster,\n",
    "    copy_xr_metadata,\n",
    "    create_sampling_mask,\n",
    "    draw_legend,\n",
    "    plot_bands,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Constants used when choosing a sampler\n",
    "DO_NOT_BALANCE = 1\n",
    "BALANCE_USING_CLASS_WEIGHT = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set working directory to the earthpy data directory\n",
    "os.chdir(os.path.join(DATA_DIR, \"woolsey-fire\"))\n",
    "\n",
    "# Set base plot style\n",
    "sns.set(font_scale=1.2, style=\"white\")\n",
    "plt.rc(\"figure.constrained_layout\", use=True, h_pad=15/72, w_pad=15/72)\n",
    "\n",
    "# Disable interpolation to prevent NaN erosion\n",
    "plt.rc(\"image\", interpolation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_format(num):\n",
    "    \"\"\"Formats float to a string for output by pandas\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num: int or float\n",
    "        number to format for print\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        number as a string\n",
    "    \"\"\"\n",
    "    return f\"{num:.3f}\" if num % 1 else str(int(num))\n",
    "\n",
    "\n",
    "def one_line_report(report, labels, metrics):\n",
    "    \"\"\"Converts a classification report to a dict\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    report: str\n",
    "        output from `sklearn.metrics.classification_report`\n",
    "    labels: list of str\n",
    "        labels targeted by classifier\n",
    "    metrics: list of str\n",
    "        columns from the classification report to capture. Metric names\n",
    "        are shortened to 2-3 character abbreviations in the output.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        dict containing key metrics for each class from the classification\n",
    "        report\n",
    "    \"\"\"\n",
    "\n",
    "    lines = iter(report.splitlines())\n",
    "    \n",
    "    # Create ordered list of column headings\n",
    "    cols = [\"accuracy\", \"pre_macro_avg\", \"rec_macro_avg\", \"f1_macro_avg\"]\n",
    "    for label in labels:\n",
    "        for metric in metrics:\n",
    "            cols.append(f\"{metric}_{label}\")\n",
    "    \n",
    "    # Shorten keys to 2-3 letter abbreviations\n",
    "    keys = [\"label\"] + [k[:3].rstrip(\"-\") for k in next(lines).split()]\n",
    "    \n",
    "    one_line = {}\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            \n",
    "            # Convert strings to floats\n",
    "            vals = [\n",
    "                float(s) if s.isnumeric() else s \n",
    "                for s in re.split(r\"  +\", line.strip())\n",
    "            ]\n",
    "            \n",
    "            # Map values to keys\n",
    "            if len(vals) == len(keys):\n",
    "                for key, val in dict(zip(keys, vals)).items():\n",
    "                    one_line[f\"{key}_{vals[0].replace(' ', '_')}\"] = val\n",
    "            elif vals[0] == \"accuracy\":\n",
    "                one_line[vals[0]] = vals[-2]\n",
    "            elif vals[0] == \"macro avg\":\n",
    "                one_line[\"pre_macro_avg\"]= vals[1]\n",
    "                one_line[\"rec_macro_avg\"]= vals[2]\n",
    "                one_line[\"f1_macro_avg\"]= vals[3]\n",
    "    \n",
    "    return {c: one_line.get(c, \"\") for c in cols}\n",
    "\n",
    "\n",
    "def prep_for_model(x, y, mask=None):\n",
    "    \"\"\"Prepares data for use in a sklearn model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: xarray.DataArray\n",
    "        an array containing the explanatory data. Can contain more than one\n",
    "        band.\n",
    "    y: xarray.DataArray\n",
    "        an array with a single band containing the response variable\n",
    "    mask: numpy.array\n",
    "        an array used to mask x and y\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of xarray.DataArray\n",
    "        x and y formatted for use in a sklean model\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    \n",
    "    # Ignore pixels set to False in the mask\n",
    "    if mask is not None:\n",
    "        x = x.where(mask)\n",
    "        y = y.where(mask)\n",
    "\n",
    "    # Convert to 1D arrays\n",
    "    x = np.array([np.ravel(b) for b in x])\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    # Limit to values that are finite in all arrays\n",
    "    xy_mask = y.copy()\n",
    "    for band in x:\n",
    "        xy_mask *= band\n",
    "\n",
    "    x = np.array([b[np.isfinite(xy_mask)] for b in x])\n",
    "    y = y[np.isfinite(xy_mask)]\n",
    "    \n",
    "    return x.transpose(), y\n",
    "\n",
    "\n",
    "def slugify(val):\n",
    "    \"\"\"Makes a string suitable for a filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    val: str\n",
    "        the string to base the filename on\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        the string as a filename\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"_\", val.lower()).strip(\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b79a7b",
   "metadata": {},
   "source": [
    "## Restore and prepare data\n",
    "\n",
    "To ensure consitency across notebooks and avoid repeating code, all data is loaded using **1-load_data.ipynb** and restored as needed in individual notebooks. Please see that notebook for details about the sources of data used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for stored ready variable and run load-data.ipynb if not found\n",
    "%store -r woolsey_data_ready\n",
    "try:\n",
    "    woolsey_data_ready\n",
    "except NameError:\n",
    "    print(\"Running load-data.ipynb...\")\n",
    "    run_notebook(\"1-load-data.ipynb\")\n",
    "\n",
    "# Retore variables using storemagic. Each variable is restored explicitly to\n",
    "# avoid confusion about where variable names are coming from.\n",
    "%store -r all_data\n",
    "%store -r reproj_to\n",
    "%store -r res\n",
    "\n",
    "%store -r hill_fire\n",
    "%store -r woolsey_fire\n",
    "%store -r hill_and_woolsey_fires\n",
    "\n",
    "%store -r cmap_dnbr\n",
    "%store -r labels_dnbr\n",
    "%store -r cmap_vegetation\n",
    "%store -r labels_vegetation\n",
    "%store -r prism_grid\n",
    "\n",
    "# Set crop and fire boundaries\n",
    "fire_name = \"Woolsey Fire\"\n",
    "fire_bound = {\"Woolsey Fire\": woolsey_fire, \"Hill Fire\": hill_fire}[fire_name]\n",
    "crop_bound = fire_bound.envelope\n",
    "\n",
    "# Create dict to store figures\n",
    "figures = {}\n",
    "\n",
    "# Set datetime param for model run\n",
    "run_datetime = datetime.now().strftime(\"%Y%m%dT%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1234c3",
   "metadata": {},
   "source": [
    "### Aggregate data and define model-only fields\n",
    "\n",
    "Model-only fields (like the blurred views of vegetation communities) are generally subsets or calculations that seemed like they might be useful for the model but didn't really work out and weren't interesting/relevant enough to add to **load-data.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ca452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup for all data\n",
    "datasets = all_data.copy()\n",
    "\n",
    "# Extract last year for fraction-alive data and derivatives\n",
    "fal = datasets[\"FAL (2013-2016)\"]\n",
    "datasets[\"FAL (2016)\"] = fal[-1]\n",
    "datasets[\"dFAL (2013-2016)\"] = fal[0] - fal[3]\n",
    "datasets[\"Dead (2016)\"] = datasets[\"Dead (2013-2016)\"][-1]\n",
    "\n",
    "# Group communities into grass/shrubs and trees\n",
    "comm = datasets[\"Community\"]\n",
    "shrubs_trees = comm.copy()\n",
    "shrubs_trees = xr.where(shrubs_trees == 2, 1, shrubs_trees)\n",
    "shrubs_trees = xr.where(shrubs_trees == 3, 1, shrubs_trees)\n",
    "shrubs_trees = xr.where(shrubs_trees == 4, 2, shrubs_trees)\n",
    "shrubs_trees = xr.where(shrubs_trees == 5, 2, shrubs_trees)\n",
    "shrubs_trees = shrubs_trees.where((shrubs_trees == 1) | (shrubs_trees == 2))\n",
    "\n",
    "shrubs_trees = shrubs_trees.rio.write_crs(datasets[\"Community\"].rio.crs)\n",
    "datasets[\"Community (Shrubs and Trees)\"] = shrubs_trees\n",
    "\n",
    "# Create blurred community maps. The hope here was that the blurred views\n",
    "# would help the model make a stronger connection between oak woodlands and\n",
    "# burn severity by getting near-oak pixels involved. Didn't really work.\n",
    "for i, name in enumerate([\n",
    "    \"Annual Grass\",\n",
    "    \"Chaparral\",\n",
    "    \"Coastal Sage Shrub\",\n",
    "    \"Oak Woodland\",\n",
    "    \"Riparian\",\n",
    "    \"Substrate\"\n",
    "]):\n",
    "    subset = xr.where(comm == i + 1, 1, 0)\n",
    "    blurred = gaussian(\n",
    "        subset.where(np.isfinite(subset), 0), sigma=3, truncate=6\n",
    "    )\n",
    "    blurred = rescale_intensity(blurred, out_range=(0, 1))\n",
    "    blurred = copy_xr_metadata(comm, blurred)\n",
    "    datasets[name] = blurred\n",
    "    \n",
    "subset = xr.where((comm == 4) | (comm == 5), 1, 0)\n",
    "blurred = gaussian(subset.where(np.isfinite(subset), 0), sigma=3, truncate=6)\n",
    "blurred = rescale_intensity(blurred, out_range=(0, 1))\n",
    "blurred = copy_xr_metadata(comm, blurred)\n",
    "datasets[\"Trees\"] = blurred\n",
    "\n",
    "# Use the mean for each pixel for each four-year set of climate data\n",
    "for key in (\n",
    "    \"Days Precipitation (2013-2016)\",\n",
    "    \"Max VPD (2013-2016)\",\n",
    "    \"Minimum Temperature (2013-2016)\",\n",
    "    \"Heat Days Over 95 (2013-2016)\",\n",
    "    \"Cumulative Precipitation (2013-2016)\"\n",
    "):\n",
    "    datasets[key] = datasets[key].sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d777f7c4",
   "metadata": {},
   "source": [
    "### Configure model parameters\n",
    "\n",
    "The constants in the cell below—`FEATURES`, `LABELS`, `CATEGORIES`, `CLASSIFIER`, `CLASSIFIER_PARAMS`, `SAMPLING PARAMS`, `BALANCE_DATA`, `DRAW_PAIR_PLOTS`, and `RUN_NAME`—are used to configure how the model will run. At the end of the notebook, the notebook saves the content of each variable along with the results of the model. Note that `fire_name` and `fire_bound` are set above to maintain consistency with other notebooks in this project. \n",
    "\n",
    "Note that the model presented here only considers pre-fire conditions. Conditions (like wind) and interventions (like the firefighting response) are not included even though they contributed to how the fire developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features by unhashing lines with strings\n",
    "FEATURES = [\n",
    "    # Vegetation\n",
    "    \"Community\",\n",
    "    #\"Community (Shrubs and Trees)\",\n",
    "    #\"Annual Grass\",\n",
    "    #\"Chaparral\",\n",
    "    #\"Coastal Sage Shrub\",\n",
    "    #\"Oak Woodland\",\n",
    "    #\"Riparian\",\n",
    "    #\"Substrate\",\n",
    "    #\"Trees\",\n",
    "    \n",
    "    # Vegetation health\n",
    "    \"FAL (2016)\",\n",
    "    #\"Dead (2016)\",\n",
    "    #\"dFAL (2013-2016)\",\n",
    "    #\"Years Dead\",\n",
    "    #\"Live FAL\",\n",
    "    \"Last Burned\",\n",
    "        \n",
    "    # Pre-fire spectral indices\n",
    "    #\"LFMC (2018)\",\n",
    "    #\"NDVI (2018)\",\n",
    "    #\"NDMI (2018)\",\n",
    "    #\"NDWI (2018)\",\n",
    "    #\"SAVI (2018)\",\n",
    "    \n",
    "    # Topography\n",
    "    \"Elevation\",\n",
    "    #\"Aspect\",\n",
    "    \"Folded Aspect\",\n",
    "    \"Slope\",\n",
    "    \n",
    "    # Climate data aggregated to water year (Oct 1-Sep 30) for 2013-2016\n",
    "    #\"Days Precipitation (2013-2016)\",\n",
    "    #\"Max VPD (2013-2016)\",\n",
    "    #\"Minimum Temperature (2013-2016)\",\n",
    "    #\"Heat Days Over 95 (2013-2016)\",\n",
    "    #\"Cumulative Precipitation (2013-2016)\",\n",
    "    \n",
    "    # Climate data aggregated to calendar year for 2018\n",
    "    \"Precipitation (2018)\",\n",
    "    #\"Mean Dew Point Temperature (2018)\",\n",
    "    #\"Maximum Temperature (2018)\",\n",
    "    #\"Mean Temperature (2018)\",\n",
    "    #\"Minimum Temperature (2018)\",\n",
    "    #\"Maximum VPD (2018)\",\n",
    "    #\"Minimum VPD (2018)\",\n",
    "    \n",
    "    # Burn severity\n",
    "    #\"MTBS dNBR\",\n",
    "    #\"MTBS Classified dNBR\",\n",
    "    #\"MTBS Classified Burned/Unburned\",\n",
    "    #\"Sentinel-2 dNBR\",\n",
    "    #\"Sentinel-2 Classified dNBR\",\n",
    "]\n",
    "\n",
    "# Response variable. Must be one of the classified dNBR plots. The MTBS\n",
    "# version is preferred.\n",
    "LABELS = \"MTBS Classified dNBR\"\n",
    "\n",
    "# Non-continuous features are handled differently by some sampling algorithms,\n",
    "# so make a list of any categorical data that may be considered by the model.\n",
    "CATEGORIES = [\n",
    "    \"Community\",\n",
    "    \"Community (Shrubs and Trees)\",\n",
    "    \"Dead (2016)\",\n",
    "    \"Years Dead\",\n",
    "    \"Last Burned\",\n",
    "    \"MTBS Classified dNBR\",\n",
    "    \"MTBS Classified Burned/Unburned\",\n",
    "    \"Sentinel-2 Classified dNBR\"\n",
    "]\n",
    "\n",
    "# Classifier class. RandomForestClassifier and GradientBoostingClassifier\n",
    "# are imported above, but really any scikit-learn classifier could be used.\n",
    "CLASSIFIER = RandomForestClassifier\n",
    "\n",
    "# Keyword arguments passed to classifier. If this dict does not include \n",
    "# n_estimators, the notebook will tune the classifier based on a set of\n",
    "# reasonable parameters using the GridSearchCV function. Tuning is slow, so\n",
    "# provide params if you can.\n",
    "CLASSIFIER_PARAMS = {\n",
    "    \n",
    "    # RandomForestClassifer params. The class_weight parameter is ignored\n",
    "    # if specified. Set BALANCE_DATA below to True if you want to rebalance\n",
    "    # imbalanced data.\n",
    "    \"oob_score\": True,\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": 5 \n",
    "    \n",
    "    # GradientBoostingClassifier params\n",
    "    #\"n_estimators\": 100,\n",
    "    #\"learning_rate\": 1.0,\n",
    "    #\"max_depth\": 1,\n",
    "    #\"random_state\": 0\n",
    "\n",
    "}\n",
    "\n",
    "# Keyword arguments pass to create_sampling_mask\n",
    "SAMPLING_PARAMS = {\n",
    "    \"counts\": {\"training\": 9000, \"validation\": 3000},\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "# Specifies whether to under/over-sample data to better model imbalanced data.\n",
    "# See \"Handled imbalanced data\" for more info on this topic.\n",
    "BALANCE_DATA = True\n",
    "\n",
    "# Specifies whether to draw pair plots. Drawing the plots is time-consuming, \n",
    "# so set to False if you want to run a bunch of different models.\n",
    "DRAW_PAIR_PLOTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165c7ac",
   "metadata": {},
   "source": [
    "### Name run and set mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb02b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign this run a name. If a subset, make sure the mask is correct.\n",
    "RUN_NAME = \"All Vegetation\"\n",
    "if BALANCE_DATA:\n",
    "    RUN_NAME += \" (Balanced)\"\n",
    "\n",
    "# Limit the model to a subset of the available pixels. This should be\n",
    "# understood as training a subset-specific model (as opposed to training a\n",
    "# model on a subset that can then be applied to the whole dataset). \n",
    "#\n",
    "# The hashed lines correspond to:\n",
    "# \n",
    "# 1. Selecting a specific community\n",
    "# 2. Selecting all grass/shrubs\n",
    "# 3. Selecting all trees\n",
    "cond = (\n",
    "    (datasets[LABELS] < 5)\n",
    "    #& (datasets[\"Community\"] == 4)\n",
    "    #& ((datasets[\"Community\"] == 1) | (datasets[\"Community\"] == 2) | (datasets[\"Community\"] == 3))\n",
    "    #& ((datasets[\"Community\"] == 4) | (datasets[\"Community\"] == 5))  3 trees\n",
    ")\n",
    "\n",
    "mask = xr.where(cond, True, False)\n",
    "mask = mask.rio.clip(fire_bound.geometry, drop=False)\n",
    "mask = mask.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089b1a9",
   "metadata": {},
   "source": [
    "### Re-use an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d25f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use an existing model. This allows you to try a model on another fire. If\n",
    "# model_dir is empty, a new model will be created below.\n",
    "model_dir = None\n",
    "if model_dir:\n",
    "    # Get path to pickled model\n",
    "    model_path = os.path.join(model_dir, \"model.pickle\")\n",
    "    \n",
    "    # Update parameters to match model. Note that some parameters, notably the\n",
    "    # sampling mask, are not captured.\n",
    "    with open(os.path.join(model_dir, \"model.json\")) as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    sampler_name = params[\"sampler\"]\n",
    "        \n",
    "    FEATURES = params[\"features\"]\n",
    "    LABELS = params[\"labels\"]\n",
    "    CLASSIFIER_PARAMS = params[\"params\"][\"classifier_params\"]\n",
    "    SAMPLING_PARAMS = params[\"params\"][\"sampling_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f22306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and verify the sampling mask\n",
    "xda = reproj_to.rio.clip(fire_bound.geometry, drop=False)\n",
    "while True:\n",
    "    try:\n",
    "        sampling_mask = create_sampling_mask(xda.copy().where(mask), **SAMPLING_PARAMS)\n",
    "        break\n",
    "    except ValueError:\n",
    "        # Too many pixels in sample. Adjust counts down until there are enough.\n",
    "        print(SAMPLING_PARAMS[\"counts\"])\n",
    "        for key, count in SAMPLING_PARAMS[\"counts\"].items():\n",
    "            SAMPLING_PARAMS[\"counts\"][key] = int(count * 0.9)\n",
    "\n",
    "# Verify that the sampling mask is the right shape\n",
    "if sampling_mask.shape[-2:] != prism_grid.shape[-2:]:\n",
    "    raise ValueError(\"Invalid shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation subsets\n",
    "training = {k: v.where(sampling_mask[0]) for k, v in datasets.items()}\n",
    "validation = {k: v.where(sampling_mask[1]) for k, v in datasets.items()}\n",
    "\n",
    "# Mask datasets\n",
    "datasets = {k: v.where(mask) for k, v in datasets.items()}\n",
    "\n",
    "# Create subset clipped to the fire boundary\n",
    "clipped = {\n",
    "    k: v.rio.clip(fire_bound.geometry, drop=False)\n",
    "    for k, v in datasets.items()\n",
    "}\n",
    "\n",
    "# Create lookup for all subsets\n",
    "subsets = {\n",
    "    \"all\": datasets,\n",
    "    \"clipped\": clipped,\n",
    "    \"training\": training,\n",
    "    \"validation\": validation,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c779b64",
   "metadata": {},
   "source": [
    "### Plot sampling mask as a map\n",
    "\n",
    "Plot a map view of the training and validation datasets. This plot mostly just confirms that the mask is behaving correctly and only pulling pixels from inside the fire scar. If the script balances the data when the sampling mask is created, you can see where pixels are being over- or under-sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34afca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sampling mask\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "plot_bands(sampling_mask[0], ax=ax1, cbar=None, title=f\"{RUN_NAME} - Training\")\n",
    "plot_bands(sampling_mask[1], ax=ax2, cbar=None, title=f\"{RUN_NAME} - Validation\")\n",
    "for ax in (ax1, ax2):\n",
    "    fire_bound.plot(ax=ax, facecolor=\"none\", edgecolor=\"gray\", linewidth=1)\n",
    "\n",
    "# Store figure to save later\n",
    "figures[\"sampling_masks\"] = fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae2369",
   "metadata": {},
   "source": [
    "## Prepare and run the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7dd2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature dataset based on keys selected near the top of the notebook\n",
    "features = sorted([f for f in FEATURES if f != LABELS])\n",
    "\n",
    "xdata = {}\n",
    "for subset, lookup in subsets.items():\n",
    "    \n",
    "    bands = []\n",
    "    for key in features:\n",
    "        band = lookup[key]\n",
    "        \n",
    "        # If more than one layer, use the last one. You can get around this\n",
    "        # behavior by selecting a layer or aggregating all layers above (for\n",
    "        # example, climate data uses the mean of the four years).\n",
    "        if len(band.shape) > 2:\n",
    "            band = band[-1]\n",
    "        \n",
    "        bands.append(band)\n",
    "        \n",
    "        try:\n",
    "            del bands[-1][\"band\"]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        bands[-1] = bands[-1].squeeze()\n",
    "        bands[-1][\"band\"] = len(bands)\n",
    "      \n",
    "    xdata[subset] = xr.concat(bands, dim=\"band\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation splits\n",
    "tx, ty = prep_for_model(xdata[\"training\"], training[LABELS])\n",
    "vx, vy = prep_for_model(xdata[\"validation\"], validation[LABELS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d8fcc0",
   "metadata": {},
   "source": [
    "### Plot histograms\n",
    "\n",
    "Plot histograms of all data and subsets. If the script balances the data when the sampling mask is created, you can these plots to verify that the bins are all the same size (but as of now that behavior is no longer supported)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed1d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same colors defined in the MTBS dNBR raster. These are defined in\n",
    "# a color map defined in load-data.ipynb, but there is no obvious way to\n",
    "# convert that map back to a list.\n",
    "dnbr_colors = [\n",
    "    \"#006400\",\n",
    "    \"#7FFFD4\",\n",
    "    \"#FFFF00\",\n",
    "    \"#FF0000\",\n",
    "    \"#7FFF00\"\n",
    "]\n",
    "\n",
    "# Save histograms of labels as PNG\n",
    "response = np.array([a[LABELS] for a in [clipped, training, validation]])\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(8, 16))\n",
    "for ax, arr, title in (zip(axes, response, [\"All\", \"Training\", \"Validation\"])):\n",
    "    \n",
    "    counts = {}\n",
    "    for val in np.unique(arr[np.isfinite(arr)]):\n",
    "        label = labels_dnbr[int(val) - 1].replace(\" \", \"\\n\", 1)\n",
    "        counts[label] = np.sum(arr == val)\n",
    "    \n",
    "    ax.bar(counts.keys(), counts.values(), color=dnbr_colors)\n",
    "    ax.set(title=f\"{RUN_NAME} - {title}\", ylabel=\"counts\")\n",
    "    \n",
    "    # Rotate labels\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(90) \n",
    "\n",
    "# Store figure to save later\n",
    "figures[\"sampling_histograms\"] = fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6001e8",
   "metadata": {},
   "source": [
    "### Tune the classifier\n",
    "\n",
    "Each of the `scikit-learn` classifiers has a boatload of hyperparameters, some more mysterious than others. The\n",
    "`GridSearchCV` methods tunes the hyperparameters using lists of possible values supplied by the user.\n",
    "\n",
    "This operation is expensive, so it only runs if the user does not supply CLASSFIER_PARAMS above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters if estimators, depth, etc. not given\n",
    "CLASSIFIER_PARAMS.setdefault(\"random_state\", 0)\n",
    "\n",
    "if \"n_estimators\" not in CLASSIFIER_PARAMS:\n",
    "    \n",
    "    # This is a simplistic tuning, but the upside is that it should work with\n",
    "    # either RandomForestClassifier or GradientBoostingClassifier.\n",
    "    param_grid = {\n",
    "        \"max_depth\": [4, 8, 16, None],\n",
    "        \"n_estimators\": [100, 200, 500],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=CLASSIFIER(),\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"f1_samples\",\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    grid_search.fit(tx, ty)\n",
    "    \n",
    "    # Update the params being passed to the classifier. You can also copy\n",
    "    # these to the \n",
    "    CLASSIFIER_PARAMS.update(grid_search.best_params_)\n",
    "    \n",
    "    display(CLASSIFIER_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35be89d",
   "metadata": {},
   "source": [
    "### Handle imbalanced data\n",
    "\n",
    "High burn severities occurred in only a small part of the Woolsey Fire. The imbalanced data poses a challenge for the random-forest model, which happily ignores the low-frequency pixels because those misses make only a small contribution to the overall accuracy of the model. When the model is trained using the original data frequencies, it fails to predict any high-severity pixels at all.\n",
    "\n",
    "Python machine-learning packages have implemented tools for working with imbalanced data, so there are options to address imbalance. The `RandomForestClassifier` in scikit-learn provides a keyword argument, `class_weight`, that allows users to rebalance data, while the imablanced-learn module provides a suite of different under- and oversampling functions to accomplish something similar. The results of early runs of the random forest model using `class_weight` weren't great, so I added the imbalanced-learn samplers to try to improve them. The current version tries out all the relevant samplers and keeps the one with the lowest F1 score.\n",
    "\n",
    "The imbalanced-learn samplers use two algorithms--the Adaptive Synthetic (ADASYN) algortim and the Synthetic Minority Over-sampling Technique (SMOTE)--to generate synthetic samples to fill out low-frequency groups. That package also implements variants on the SMOTE algorithm that combine under- and over-sampling techniques (like SMOTEENN and SMOTETomek) or are suitable for a mix of categorical and continuous data (SMOTENC). The [documentation for imbalanced-learn](https://imbalanced-learn.org/dev/introduction.html) has more info about the different samplers, including comparisons between them.\n",
    "\n",
    "I have not tried to tune the samplers, but in general, `class_weight` performs about as well as any of them.\n",
    "\n",
    "\n",
    "#### Evaluating models based on imbalanced data\n",
    "\n",
    "Precision, recall, and F1 scores for indivudal classes are useful for evaluating models trained using imbalanced data, which may have high overall accuracy while completely missing classes that appear infrequently. In scikit-learn, all three scores can be calculated using `sklearn.metrics.classification_report`. In imbalanced-learn, these plus a handful of other scores can be calculated using `imblearn.metrics.classification_report_imbalanced`.\n",
    "\n",
    "When running a classifier, each class can be understood as its own binary system. A given pixel\n",
    "\n",
    "1. is either part of that class or not\n",
    "2. is either classified as that class or not\n",
    "\n",
    "Within this framework, there are four possible outcomes for class A:\n",
    "\n",
    "+ **True positive:** A is classified as A\n",
    "+ **False positive/:** ~A is classified as A (error of commission)\n",
    "+ **False negative:** A is classified as ~A (error of omission)\n",
    "+ **True negative:** ~A is classified as ~A\n",
    "\n",
    "These outcomes can be used to calculate precision and recall scores for each class:\n",
    "\n",
    "+ **Precision** is calculated as **true positive / (true positive + false\n",
    "  positive)** and is the percentage of pixels *classified as A* that are\n",
    "  *actually in class A*. Precision is the same as user's accuracy. \n",
    "\n",
    "\n",
    "+ **Recall** is calculated as **true positive / (true positive + false negative)**\n",
    "  and is the percentage of pixels *acutally in class A* that were *classified as\n",
    "  A*. Recall is the same as producer's accuracy.\n",
    "\n",
    "\n",
    "+ The **F1 score** combines precision and recall into a single metric by taking\n",
    "  the harmonic mean of the two values: **2 * (precision * recall) / (precision +\n",
    "  recall)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model. Use a loop so that the model can be run  multiple times to assess\n",
    "# certain performance metrics, if desired. \n",
    "runs = []\n",
    "for i in range(1):\n",
    "    \n",
    "    # Identify categorical features. Categories do not play well with some\n",
    "    # sampling algorithms, so use different samplers if categories are usedS.\n",
    "    cat_features = [i for i, f in enumerate(features) if f in CATEGORIES]\n",
    "    \n",
    "    # Both list of samplers contain two constants as the first two values in\n",
    "    # the list tell the script to (1) not balance the data at all and (2) use\n",
    "    # the class_weight argument in RandomForestClassifier to balance the data.\n",
    "    if cat_features:\n",
    "        samplers = [\n",
    "            DO_NOT_BALANCE,\n",
    "            BALANCE_USING_CLASS_WEIGHT,\n",
    "            SMOTENC(categorical_features=cat_features, random_state=0)\n",
    "        ]\n",
    "    else:\n",
    "        samplers = [\n",
    "            DO_NOT_BALANCE,\n",
    "            BALANCE_USING_CLASS_WEIGHT,\n",
    "            ADASYN(sampling_strategy=\"all\", random_state=0),\n",
    "            SMOTE(random_state=0),\n",
    "            SMOTEENN(random_state=0),\n",
    "            SMOTETomek(random_state=0),\n",
    "        ]\n",
    "    \n",
    "    # Try each sampler to see which one yields the highest f-score\n",
    "    scored = {}\n",
    "    for sampler in samplers:\n",
    "        if model_dir:\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                pipeline = pickle.load(f)\n",
    "        elif sampler == DO_NOT_BALANCE:\n",
    "            sampler_name = \"unbalanced\"\n",
    "            pipeline = make_pipeline(CLASSIFIER(**CLASSIFIER_PARAMS))\n",
    "        elif sampler == BALANCE_USING_CLASS_WEIGHT:\n",
    "            # Skip class_weight if not using RandomForestClassifier\n",
    "            if CLASSIFIER != RandomForestClassifier:\n",
    "                continue\n",
    "            sampler_name = \"class_weight\"\n",
    "            params = CLASSIFIER_PARAMS.copy()\n",
    "            params.update(class_weight=\"balanced\")\n",
    "            pipeline = make_pipeline(CLASSIFIER(**params))\n",
    "        else:\n",
    "            sampler_name = sampler.__class__.__name__\n",
    "            pipeline = make_pipeline(sampler, CLASSIFIER(**CLASSIFIER_PARAMS))\n",
    "        \n",
    "        # Fit the model\n",
    "        try:\n",
    "            if not model_dir:\n",
    "                pipeline.fit(tx, ty)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        else:\n",
    "            # Rank models by the F1 score for high-severity fires\n",
    "            observed = np.where(vy == 4, 1, 0)\n",
    "            predicted = np.where(pipeline.predict(vx) == 4, 1, 0)\n",
    "            score = f1_score(observed, predicted, average=\"binary\")\n",
    "\n",
    "            scored[score] = (sampler_name, pipeline)\n",
    "        \n",
    "        # The first entry in each list does not rebalance the data, so if\n",
    "        # the BALANCE_DATA flag evaluates to False, the iterator stops here. \n",
    "        if not model_dir and not BALANCE_DATA:\n",
    "            break\n",
    "        \n",
    "    # Use the classifier with the best results (here, the lowest F1 score)\n",
    "    sampler_name, classifier = scored[sorted(scored.keys())[-1]]\n",
    "    \n",
    "    # Print results of the model. Suppress warnings about ill-defined labels.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        \n",
    "        # Limit labels to those present in the dataset\n",
    "        labels = [\"unburned\", \"low\", \"moderate\", \"high\", \"enhanced regrowth\"]\n",
    "        predicted = classifier.predict(vx)\n",
    "        vals = set(list(np.unique(vy)) + list(np.unique(predicted)))\n",
    "        labels = [lbl for i, lbl in enumerate(labels) if (i + 1) in vals]\n",
    "        \n",
    "        report = classification_report(\n",
    "            vy, classifier.predict(vx), target_names=labels\n",
    "        )\n",
    "    \n",
    "    print(f\"Sampler: {sampler_name}\")\n",
    "    print(report)\n",
    "    \n",
    "    # Summarize the run as a list\n",
    "    run = []\n",
    "    \n",
    "    # Get the OOB score if using RandomForestClassifier\n",
    "    try:\n",
    "        run.append(classifier.steps[-1][-1].oob_score_)\n",
    "    except AttributeError:\n",
    "        run.append(0)\n",
    "    run.append(mean_squared_error(vy, classifier.predict(vx)) ** 0.5)\n",
    "    \n",
    "    # Add precision, recall, and related metrics extracted from report\n",
    "    rpt = one_line_report(\n",
    "        report, [\"unburned\", \"low\", \"moderate\", \"high\"], [\"pre\", \"rec\", \"f1\"]\n",
    "    )\n",
    "    run.extend(rpt.values())\n",
    "    \n",
    "    # Add feature importances\n",
    "    run.extend(classifier.steps[-1][-1].feature_importances_)\n",
    "    \n",
    "    runs.append(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ab882",
   "metadata": {},
   "source": [
    "### Summarize and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bdb15a",
   "metadata": {},
   "source": [
    "The model results include three metrics beyond the precision-and-recall suite defined above:\n",
    "\n",
    "+ The **out-of-bag score** is a cross-validation metric calculated by\n",
    "  RandomForestClassifier as it goes. Each tree created by the classifier uses\n",
    "  only a subset of the available training data, and the out-of-bag score is\n",
    "  calculated by comparing the predictions of trees trained with and without each\n",
    "  subset. Lower values are better.\n",
    "\n",
    "\n",
    "+ **Root mean square error** estimates the average standard deviation between\n",
    "  predicted and observed values. Lower values are better.\n",
    "\n",
    "\n",
    "+ **Feature importance** estimates how important each feature was to the\n",
    "  predictions made by the model. Higher values indicate higher importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results from each run in a dataframe with many, many columns. The\n",
    "# results of the last run are the ones that get stored at the end of the\n",
    "# notebook.\n",
    "\n",
    "# Get a list of feature columns\n",
    "feat_cols = {f: slugify(f\"feat_imp_{f}\") for f in features}\n",
    "\n",
    "cols = [\"oob\", \"rmse\"] + list(rpt) + list(feat_cols.values())\n",
    "results = pd.DataFrame(runs, columns=cols)\n",
    "\n",
    "# Map feature names to importances\n",
    "feat_imp = {f: getattr(results.iloc[-1], c) for f, c in feat_cols.items()}\n",
    "\n",
    "# Transpose results for display\n",
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(results.round(3).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfec894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the results from the separate runs if multiple. No output from\n",
    "# this cell if only one row.\n",
    "if len(results) > 1:\n",
    "    agg_results = results.agg([\"mean\", \"std\", \"min\", \"max\", \"count\"])\n",
    "    \n",
    "    # Transpose results for display\n",
    "    with pd.option_context(\"display.max_columns\", None):\n",
    "        display(agg_results.round(3).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc55b70",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "The confusion matrix illustrates how values predicted by the model compare to the observed values. The rows contain the predicted values and the columns the observed values for each class. The diagonal shows the number of pixels where predicted matches observed. The precision and recall scores described above summarize the information presented in the confusion matrix. The matrix itself is useful for evaluating which classes are being confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the confusion matrix\n",
    "conf_mtx = pd.DataFrame({\"observed\": vy, \"predicted\": classifier.predict(vx)})\n",
    "\n",
    "# Cross-tabulate predictions\n",
    "crosstab = pd.crosstab(conf_mtx[\"observed\"], conf_mtx[\"predicted\"], margins=True)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea80b3",
   "metadata": {},
   "source": [
    "## Apply the model to the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780890be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the trained model to the full dataset. Note that this calculation\n",
    "# excludes any pixels that were excluded from the model.\n",
    "all_x, all_y = prep_for_model(xdata[\"clipped\"], subsets[\"clipped\"][LABELS])\n",
    "predicted = classifier.predict(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification report based on the full dataset. Originally added\n",
    "# to help assess datasets where oversampling was done via the sampling mask,\n",
    "# now just here as a sanity check.\n",
    "\n",
    "# Limit labels to those present in the dataset\n",
    "labels = [\"unburned\", \"low\", \"moderate\", \"high\", \"enhanced regrowth\"]\n",
    "vals = set(list(np.unique(all_y)) + list(np.unique(predicted)))\n",
    "labels = [lbl for i, lbl in enumerate(labels) if (i + 1) in vals]\n",
    "\n",
    "# Create classification report. Suppress warnings about ill-defined labels.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")    \n",
    "    report_all = classification_report(all_y, predicted, target_names=labels)\n",
    "    \n",
    "print(report_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214f0f6",
   "metadata": {},
   "source": [
    "### Plot model results as a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload all x data. The predict method can't handle NaNs, but we can't drop \n",
    "# pixels like above because we need to reshape to the original array to plot.\n",
    "# Set NaNs to -9999 instead.\n",
    "all_x = np.array([np.ravel(x) for x in xdata[\"clipped\"]])\n",
    "all_x[~np.isfinite(all_x)] = -9999\n",
    "predicted_ = classifier.predict(all_x.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba0630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plottable versions of the observed and predicted data    \n",
    "observed = subsets[\"clipped\"][LABELS].copy()\n",
    "observed = copy_xr_metadata(subsets[\"clipped\"][LABELS], observed)\n",
    "observed = observed.rio.clip(fire_bound.geometry)\n",
    "\n",
    "predicted = predicted_.reshape(*subsets[\"clipped\"][LABELS].shape)\n",
    "predicted = copy_xr_metadata(subsets[\"clipped\"][LABELS], predicted)\n",
    "predicted = predicted.rio.clip(fire_bound.geometry)\n",
    "predicted = predicted.where(np.isfinite(observed))\n",
    "\n",
    "# Aggregate to PRISM grid. Use primarily if you need to compare burn\n",
    "# severity to the aggregated pair plot but otherwise should be hashed.\n",
    "#observed = agg_to_raster(observed, prism_grid).rio.write_crs(observed.rio.crs)\n",
    "#predicted = agg_to_raster(predicted, prism_grid).rio.write_crs(predicted.rio.crs)\n",
    "\n",
    "# Sort features based on importance\n",
    "feat_sorted = [\n",
    "    k for k, v in sorted(feat_imp.items(), key=lambda kv: -kv[1]) if v > 0.05    \n",
    "]\n",
    "\n",
    "# Set vmin and vmax so the whole range appears\n",
    "vmin = 1\n",
    "vmax = 5\n",
    "\n",
    "# Set color map based on response variable in LABELS\n",
    "cmap = cmap_dnbr\n",
    "labels = labels_dnbr\n",
    "classes = range(5)\n",
    "if \"Burned\" in LABELS:\n",
    "    cmap = ListedColormap(dnbr_colors[1:3])\n",
    "    labels = [\"Unburned/Low\", \"Moderate/High\"]\n",
    "    classes = range(2)\n",
    "\n",
    "# Draw comparison plot. Note top five features\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "fig.suptitle(\n",
    "    f\"{RUN_NAME} - {CLASSIFIER.__name__} - {LABELS} - {res} m\\n\"\n",
    "    f\"Features: {' > '.join(feat_sorted)}\"\n",
    ")\n",
    "\n",
    "plot_bands(observed,\n",
    "           ax=ax1,\n",
    "           title=\"Observed - MTBS\",\n",
    "           cmap=cmap,\n",
    "           vmin=vmin,\n",
    "           vmax=vmax,\n",
    "           cbar=False,\n",
    "           scale=False)\n",
    "\n",
    "plot_bands(predicted,\n",
    "           ax=ax2,\n",
    "           title=\"Predicted\",\n",
    "           cmap=cmap,\n",
    "           vmin=vmin,\n",
    "           vmax=vmax,\n",
    "           cbar=False,\n",
    "           scale=False)\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    fire_bound.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\", linewidth=1)\n",
    "\n",
    "# Position legend based on which fire is being plotted (Woolsey or Hill)\n",
    "if fire_name == \"Woolsey Fire\":\n",
    "    ax = ax1\n",
    "    bbox = (0.01, 0.99)\n",
    "elif fire_name == \"Hill Fire\":\n",
    "    ax = ax2\n",
    "    bbox=(0.71, 0.1)\n",
    "\n",
    "# Draw legend\n",
    "draw_legend(im_ax=ax.get_images()[0],\n",
    "            classes=classes,\n",
    "            titles=labels,\n",
    "            bbox=bbox)\n",
    "\n",
    "# Store figure to save later\n",
    "figures[\"map\"] = fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93672613",
   "metadata": {},
   "source": [
    "## Save parameters and results\n",
    "\n",
    "Each model run is saved in the **outputs/models/{run_datetime}**. High-level statistics from each run are saved to **outputs/models/results.csv** to make it possible to do quick comparisons between different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739275d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect parameters in a dict\n",
    "model = {\n",
    "    \"name\": RUN_NAME,\n",
    "    \"classifier\": CLASSIFIER.__name__,\n",
    "    \"sampler\": None if sampler is None else sampler.__class__.__name__,\n",
    "    \"features\": features,\n",
    "    \"labels\": LABELS,\n",
    "    \"params\": {\n",
    "        \"classifier_params\": CLASSIFIER_PARAMS,\n",
    "        \"sampling_params\": SAMPLING_PARAMS}\n",
    "}\n",
    "\n",
    "# Note existing model if used\n",
    "if model_dir:\n",
    "    model[\"model_dir\"] = model_dir\n",
    "\n",
    "# Set the run name to the date/time of the run\n",
    "models_path = os.path.join(\"outputs\", \"models\")\n",
    "outdir = os.path.join(models_path, f\"{slugify(RUN_NAME)}_{run_datetime}\")\n",
    "\n",
    "try:\n",
    "    os.makedirs(outdir)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4bd795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model itself\n",
    "with open(os.path.join(outdir, \"model.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(classifier, f)\n",
    "\n",
    "# Save validation results as JSON\n",
    "results_ = {\"model_id\": os.path.basename(outdir)}\n",
    "results_.update({\n",
    "    k: f\"{float(v):.3f}\" if v else \"\" \n",
    "    for k, v in results.iloc[-1].to_dict().items()\n",
    "})\n",
    "with open(os.path.join(outdir, \"results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_, f, indent=2)\n",
    "    \n",
    "# And also as text\n",
    "with open(os.path.join(outdir, \"report.txt\"), \"w\") as f:\n",
    "    f.write(report)\n",
    "    \n",
    "# Save model parameters\n",
    "with open(os.path.join(outdir, \"model.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(model, f, indent=2)\n",
    "\n",
    "# Save figures\n",
    "for fn, fig in figures.items():\n",
    "    fig.savefig(os.path.join(outdir, f\"{fn}.png\"), bbox_inches=\"tight\")\n",
    "figures = {}\n",
    "\n",
    "# Save crosstab and result tables as HTML\n",
    "css = (\n",
    "    \"<style>\"\n",
    "    \"table {border-collapse: collapse;}\"\n",
    "    \"td, th { text-align: center; padding: 8px; }\"\n",
    "    \"th { background-color: #eee; }\"\n",
    "    \"</style>\"\n",
    ")\n",
    "\n",
    "html = crosstab.transpose().to_html(float_format=float_format)\n",
    "with open(os.path.join(outdir, \"crosstab.htm\"), \"w\") as f:\n",
    "    f.write(css + \"\\n\" + html)\n",
    "\n",
    "try:\n",
    "    html = agg_results.transpose().to_html(float_format=float_format)\n",
    "except NameError:\n",
    "    html = results.transpose().to_html(float_format=float_format)\n",
    "with open(os.path.join(outdir, \"results.htm\"), \"w\") as f:\n",
    "    f.write(css + \"\\n\" + html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78366a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize results from all runs in models directory in a CSV. This is easier\n",
    "# than adding models one at a time because fields may differ between runs.\n",
    "results = []\n",
    "for root, dirs, files in os.walk(models_path):\n",
    "    for fn in files:\n",
    "        if fn == \"results.json\":\n",
    "            with open(os.path.join(root, fn)) as f:\n",
    "                results.append(json.load(f))\n",
    "\n",
    "results.sort(key=len, reverse=True)\n",
    "keys = {}\n",
    "for result in results:\n",
    "    keys.update(result)\n",
    "keys = list(keys)\n",
    "keys = (\n",
    "    [k for k in keys if not k.startswith(\"feat_\")] + \\\n",
    "    sorted([k for k in keys if k.startswith(\"feat_\")])\n",
    ")\n",
    "    \n",
    "path = os.path.join(models_path, \"results.csv\")\n",
    "with open(path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, dialect=\"excel\")\n",
    "    writer.writerow(keys)\n",
    "    for row in sorted(results, key=lambda r: r[\"model_id\"]):\n",
    "        writer.writerow([row.get(k, \"\") for k in keys])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b2375",
   "metadata": {},
   "source": [
    "## Evaluate source data using pair plots\n",
    "\n",
    "Pair plots can be time-consuming to generate and can offer insight into areas where the model went wrong, so I've saved them for last. Use the pair plots below to look for:\n",
    "\n",
    "1. **Collinear features.** Use this plot to assess whether to remove a given\n",
    "   feature from the model.\n",
    "2. **Clusters of misclassified features.** Use these plots to  work out why and\n",
    "   how pixels are being misclassified.  \n",
    "   \n",
    "Pair plots are also saved to the **models** folder for the current run.\n",
    "\n",
    "The first pair plot shows the source data for the model aggreated to the PRISM grid (4 km blocks). This helps clear out some of the noise in the point data and aligns the high-resolution data to the climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f974b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "# For the first plot, aggreagate the original data to the PRISM grid before\n",
    "# plotting. This reduces the noise seen on the point plots. Use the\n",
    "# unclassified version of the label for all aggregations except mode.\n",
    "\n",
    "func = np.nanmean  # for mode: lambda a: mode(a).mode\n",
    "\n",
    "yname = LABELS\n",
    "kind = \"mode\"\n",
    "if func.__name__ != \"<lambda>\":\n",
    "    yname = LABELS.replace(\"Classified \", \"\")\n",
    "    kind = \"mean\"\n",
    "\n",
    "x = xdata[\"clipped\"]\n",
    "y = subsets[\"clipped\"][yname]\n",
    "\n",
    "# Aggregate each array to the PRISM grid and append\n",
    "x = np.array([np.ravel(aggregate(b, prism_grid, func=func)) for b in x]).transpose()\n",
    "y = np.ravel(aggregate(y, prism_grid, func=func))\n",
    "\n",
    "pairs.append((x, y, yname, f\"All source data aggregated to PRISM grid ({kind})\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6f644",
   "metadata": {},
   "source": [
    "The rest of the pair plots look at pixels that have been misclassified. The burn-severity classes overlap significantly, and these plots have mostly not been useful for developing strategies to separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b192c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit output to misclassified pixels\n",
    "misses = {\n",
    "    \"unburned\": predicted == 1,\n",
    "    \"low severity\": predicted == 2,\n",
    "    \"moderate severity\": predicted == 3,\n",
    "    \"high severity\": predicted == 4\n",
    "}\n",
    "\n",
    "for key, missed in misses.items():\n",
    "    mask_ = mask * (observed != predicted) * missed\n",
    "    x, y = prep_for_model(xdata[\"training\"], subsets[\"training\"][LABELS], mask_)\n",
    "    \n",
    "    # Add the pairs only if there is any data to plot\n",
    "    if np.isfinite(y).any():\n",
    "        pairs.append(\n",
    "            (x, y, f\"Classified dNBR\", f\"Pixels misclassified as {key}\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw pair plots if desired\n",
    "if DRAW_PAIR_PLOTS:\n",
    "    \n",
    "    # Pair plots include yellow glyphs, so use a black background.\n",
    "    sns.set(style=\"ticks\", context=\"talk\")\n",
    "    plt.style.use(\"dark_background\")\n",
    "\n",
    "    # Draw pair plots\n",
    "    for x, y, yname, title in pairs:\n",
    "\n",
    "        paired = pd.DataFrame(data=x, columns=features)\n",
    "        paired[yname] = y\n",
    "\n",
    "        # Suppress warnings from pairplot, which can throw a lot of warnings\n",
    "        # depending on the source data.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "            # Define palette based on whether response is categorical \n",
    "            # or continuous\n",
    "            if \"Classified\" in yname:\n",
    "                palette = [\n",
    "                    dnbr_colors[int(i - 1)] for i\n",
    "                    in np.unique(y[np.isfinite(y)])\n",
    "                ]  \n",
    "            else:\n",
    "                palette = \"RdYlGn_r\"\n",
    "\n",
    "            grid = sns.pairplot(paired,\n",
    "                                height=5,\n",
    "                                plot_kws={\"s\": 200},\n",
    "                                hue=yname,\n",
    "                                palette=palette)\n",
    "\n",
    "            grid.fig.suptitle(title, y=1.02)\n",
    "\n",
    "            # The output dir is set now, so save the figure\n",
    "            grid.fig.savefig(os.path.join(outdir, f\"pp_{slugify(title)}.png\"),\n",
    "                             bbox_inches=\"tight\")\n",
    "\n",
    "    # Reset styles\n",
    "    sns.set(font_scale=1.5, style=\"white\")\n",
    "    plt.rcParams.update(plt.rcParamsDefault)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "206px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
